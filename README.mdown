# DenMARK-contacts

This application was part of the [Denver MarkLogic Users Group October 2015 Meetup](http://www.meetup.com/den-mark-logic/events/225594479/).

The goal of this application is to show how document data may be augmented with semantics.  If you follow these steps,
you'll build a an application with contact data generated from the team's Slush Generator.  You'll ingest RDF triples
using MLCP, link the document data to the semantic data, and explore roles for people at a recent semantic web
conference.


This application was generated by the MarkLogic-Node [Slush](https://github.com/klei/slush) generator, with the following components:

- [AngularJS](https://angularjs.org/)
- [Gulp](http://gulpjs.com/)
- [node.js](http://nodejs.org/): very thin layer, hosting the Angular code and proxying MarkLogic REST API requests
- [Roxy Deployer](https://github.com/marklogic/roxy): bootstrap MarkLogic databases, application servers, etc; scaffolding for MarkLogic REST API service extensions

## Install Required Dependencies

- [node.js](http://nodejs.org/download/)
- [npm](https://www.npmjs.com/): Built-in package manager for node (comes with
  node, but check to be sure you have latest version: `npm install -g npm`)
- [gulp](http://gulpjs.com/): Javascript task automation (`npm install -g
  gulp`)
- [Bower](http://bower.io/): A package manager for front-end libraries (`npm
  install -g bower`)
- [Git](https://git-scm.com/) - Roxy depends on this version control system
- [Ruby](https://www.ruby-lang.org/en/documentation/installation/) - Roxy
  depends on Ruby in order to run server configuration scripts

# Running the application

    ./ml local bootstrap
    ./ml local deploy modules

On Windows, that would be:

    ml.bat local bootstrap
    ml.bat local deploy modules

Install additional dependencies using the bower package manager:

    bower install

If you want to upload a dictionary for spell-suggestions:

    ./ml local deploy content

On Windows:

    ml.bat local deploy content

Edit `./local.json` to set your desired ports

    gulp serve-local # this will watch the .less file for changes, compile them to .css, and run the node server

# Data

## Sample Data

The application comes with 567 JSON documents generated by json-generator.com.
You can load them with
(MLCP)[https://docs.marklogic.com/guide/ingestion/content-pump] like this:

    ./ml local mlcp -options_file import-sample-data.options

Or on Windows:

    ml.bat local mlcp -options_file import-sample-data.options

## General Data Information

The application assumes that you're storing JSON data. This shows up in the
default format request for the MLRest service's `searchContext`, the
`detailController`'s (`detail-ctrl.js`) request to get a document, and in the
out-of-the-box detail view.

## RDF Data

As part of the demo run during the DenMARK Denver MarkLogic Users Group Meetup, a sample RDF dataset
was ingested via mlcp.  This data is from the [Semantic Web Dogfood Corpus](http://data.semanticweb.org),
specifically from the ESWC 2015.

    ./ml local mlcp -options_file import-rdf.options


## Link the Datasets

Now to create the "bridge" between the JSON Documents and the RDF Data, we generate a single triple
that relates a doc's GUID property to the IRI for a person.  This work is done via a Query Console Session.

1. Navigate to (http://localhost:8000/qconsole), or the appropriate hostname for your environment.
2. From the right navigation area, click the arrow drop-down and select Import Workspace.
3. Follow the prompts, and load Workspace.xml located at the root of this repo.
4. Click the Link the Data tab and click Run.

# Recreate the demo with a demo script

Demo-Script.txt includes the steps @mariannemyers ran through live in person.  Feel free to use these
steps to recreate the demo yourself.

# Disclaimer - Dummy Data

In step #4 above, you linked the representation of your contact to a real person who is associated with
ESWC 2015.  This link was generated randomly, and the name in the document data does not match the person
who's roles are listed.  Name were not changed due to time limitations, but the idea of linking the data
through a single triple for each document is a concept that can be used in many situations.

